{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rihabidm/Neural-network-for-classification/blob/main/NN_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1ST****using only 2 classes"
      ],
      "metadata": {
        "id": "ep3Hg1PMgmST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JO8WUk9FQy6",
        "outputId": "8a1584e7-0728-488b-d983-ccb22caeb38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.098287385359733\n",
            "Epoch 10000, Loss: 2.373621016724042e-08\n",
            "Epoch 20000, Loss: 1.7907677682584455e-09\n",
            "Epoch 30000, Loss: 1.8757924244474617e-10\n",
            "Epoch 40000, Loss: 1.9678168494525928e-11\n",
            "Epoch 50000, Loss: 2.0652661591846734e-12\n",
            "Epoch 60000, Loss: 2.1678467418749998e-13\n",
            "Epoch 70000, Loss: 2.2756258773464285e-14\n",
            "Epoch 80000, Loss: 2.3887986098510204e-15\n",
            "Epoch 90000, Loss: 2.507611656700408e-16\n",
            "Test Accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Filter the classes to perform binary classification (0 for \"setosa\" and 1 for \"versicolor\")\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Normalize the data\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Convert labels to binary (0 or 1)\n",
        "y = (y == 0).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Step 3: Define the architecture\n",
        "input_size = 4\n",
        "hidden_size = 6  # 6 neurons in the hidden layer\n",
        "output_size = 1\n",
        "\n",
        "# Step 4: Initialize weights and biases\n",
        "np.random.seed(0)\n",
        "w1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "w2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Step 5: Define the activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Step 6: Forward propagation with ReLU activation\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, w1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, w2) + b2\n",
        "    a2 = relu(z2)  # Linear activation in the output layer\n",
        "    return a1, a2\n",
        "\n",
        "# Step 7: Define the loss function (Mean Squared Error)\n",
        "def compute_loss(y, y_pred):\n",
        "    n = 100\n",
        "    loss = (1 / (2 * n)) * np.sum((y_pred - y) ** 2)\n",
        "    return loss\n",
        "\n",
        "# Step 8: Backpropagation with ReLU\n",
        "def backward(X, y, a1, a2):\n",
        "    n = 100\n",
        "    dz2 = a2 - y\n",
        "    dw2 = (1/n) * np.dot(a1.T, dz2)\n",
        "    db2 = (1/n) * np.sum(dz2, axis=0, keepdims=True)\n",
        "    dz1 = np.dot(dz2, w2.T) * (a1 > 0)  # ReLU derivative\n",
        "    dw1 = (1/n) * np.dot(X.T, dz1)\n",
        "    db1 = (1/n) * np.sum(dz1, axis=0, keepdims=True)\n",
        "    return dw1, db1, dw2, db2\n",
        "\n",
        "# Step 9: Training loop\n",
        "learning_rate = 0.1\n",
        "num_epochs = 100000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        x_sample = X_train[i:i+1]\n",
        "        y_sample = y_train[i]\n",
        "\n",
        "        # Forward pass\n",
        "        a1, a2 = forward(x_sample)\n",
        "\n",
        "        # Compute and accumulate loss\n",
        "        loss = compute_loss(y_sample, a2)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation\n",
        "        dw1, db1, dw2, db2 = backward(x_sample, y_sample, a1, a2)\n",
        "\n",
        "        # Update weights and biases\n",
        "        w1 -= learning_rate * dw1\n",
        "        b1 -= learning_rate * db1\n",
        "        w2 -= learning_rate * dw2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        " # Print average loss for the epoch\n",
        "    if epoch % 10000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(X_train)}\")\n",
        "\n",
        "# Step 10: Evaluation\n",
        "_, y_pred_test = forward(X_test)  # Discard a1, keep only a2\n",
        "y_pred_test = (y_pred_test >= 0.5).astype(int)  # Convert to binary predictions\n",
        "accuracy = np.mean(y_pred_test == y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2ND****using the 3 classes"
      ],
      "metadata": {
        "id": "13ZWTKL3giRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Normalize the data\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Step 3: Define the architecture\n",
        "input_size = 4\n",
        "hidden_size = 6  # 6 neurons in the hidden layer\n",
        "output_size = 3  # Three classes in the Iris dataset\n",
        "\n",
        "# Step 4: Initialize weights and biases\n",
        "np.random.seed(0)\n",
        "w1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "w2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Step 5: Define the activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Step 6: Forward propagation with Softmax activation for multi-class classification\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, w1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, w2) + b2\n",
        "    # Softmax activation in the output layer for multi-class classification\n",
        "    a2 = np.exp(z2) / np.exp(z2).sum(axis=1, keepdims=True)\n",
        "    return a1, a2\n",
        "\n",
        "# Step 7: Define the loss function (Categorical Cross-Entropy)\n",
        "def compute_loss(y, y_pred):\n",
        "    n = len(y)\n",
        "    loss = -1/n * np.sum(y * np.log(y_pred))\n",
        "    return loss\n",
        "\n",
        "# Step 8: Backpropagation with Softmax\n",
        "def backward(X, y, a1, a2):\n",
        "    n = len(y)\n",
        "    dz2 = a2 - y\n",
        "    dw2 = (1/n) * np.dot(a1.T, dz2)\n",
        "    db2 = (1/n) * np.sum(dz2, axis=0, keepdims=True)\n",
        "    dz1 = np.dot(dz2, w2.T) * (a1 > 0)\n",
        "    dw1 = (1/n) * np.dot(X.T, dz1)\n",
        "    db1 = (1/n) * np.sum(dz1, axis=0, keepdims=True)\n",
        "    return dw1, db1, dw2, db2\n",
        "\n",
        "# Step 9: Training loop\n",
        "learning_rate = 0.1\n",
        "num_epochs = 10000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        x_sample = X_train[i:i+1]\n",
        "        y_sample = np.zeros((1, output_size))\n",
        "        y_sample[0, y_train[i]] = 1  # One-hot encoding for multi-class\n",
        "\n",
        "        # Forward pass\n",
        "        a1, a2 = forward(x_sample)\n",
        "\n",
        "        # Compute and accumulate loss\n",
        "        loss = compute_loss(y_sample, a2)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation\n",
        "        dw1, db1, dw2, db2 = backward(x_sample, y_sample, a1, a2)\n",
        "\n",
        "        # Update weights and biases\n",
        "        w1 -= learning_rate * dw1\n",
        "        b1 -= learning_rate * db1\n",
        "        w2 -= learning_rate * dw2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(X_train)}\")\n",
        "\n",
        "# Step 10: Evaluation\n",
        "y_pred_test = np.argmax(forward(X_test)[1], axis=1)  # Use argmax to get class predictions\n",
        "accuracy = np.mean(y_pred_test == y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKjougOpH7C7",
        "outputId": "e6f6a8e7-90d8-49a7-91aa-cc64faa56a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6248676885157611\n",
            "Epoch 1000, Loss: 0.0003863192635123216\n",
            "Epoch 2000, Loss: 0.00014616840572467625\n",
            "Epoch 3000, Loss: 8.825028201565847e-05\n",
            "Epoch 4000, Loss: 6.26600336260446e-05\n",
            "Epoch 5000, Loss: 4.862606080420903e-05\n",
            "Epoch 6000, Loss: 3.942353117394077e-05\n",
            "Epoch 7000, Loss: 3.30270887713574e-05\n",
            "Epoch 8000, Loss: 2.8449970474067368e-05\n",
            "Epoch 9000, Loss: 2.4734575832212192e-05\n",
            "Test Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3RD**"
      ],
      "metadata": {
        "id": "v-cmVGk-gV8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Filter the classes to perform binary classification (0 for \"setosa\" and 1 for \"versicolor\")\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Normalize the data\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Convert labels to binary (0 or 1)\n",
        "y = (y == 0).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Step 3: Define the architecture\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 1\n",
        "\n",
        "# Step 4: Initialize weights and biases\n",
        "np.random.seed(0)\n",
        "w1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "w2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Batch normalization parameters\n",
        "gamma1 = np.ones((1, hidden_size))\n",
        "beta1 = np.zeros((1, hidden_size))\n",
        "gamma2 = np.ones((1, output_size))\n",
        "beta2 = np.zeros((1, output_size))\n",
        "\n",
        "# Step 5: Define the activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Step 6: Forward propagation with batch normalization\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, w1) + b1\n",
        "    # Batch normalization for the hidden layer\n",
        "    mean1 = np.mean(z1, axis=0, keepdims=True)\n",
        "    var1 = np.var(z1, axis=0, keepdims=True)\n",
        "    z1_hat = (z1 - mean1) / np.sqrt(var1 + 1e-8)\n",
        "    z1_tilde = gamma1 * z1_hat + beta1\n",
        "    a1 = relu(z1_tilde)\n",
        "    z2 = np.dot(a1, w2) + b2\n",
        "    # Batch normalization for the output layer\n",
        "    mean2 = np.mean(z2, axis=0, keepdims=True)\n",
        "    var2 = np.var(z2, axis=0, keepdims=True)\n",
        "    z2_hat = (z2 - mean2) / np.sqrt(var2 + 1e-8)\n",
        "    z2_tilde = gamma2 * z2_hat + beta2\n",
        "    a2 = z2_tilde\n",
        "    return a1, a2\n",
        "\n",
        "# Step 7: Define the loss function (Mean Squared Error)\n",
        "def compute_loss(y, y_pred):\n",
        "    m = 100\n",
        "    loss = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "CaCzXBX4Mn1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Backpropagation\n",
        "def backward(X, y, a1, a2):\n",
        "    m = 100\n",
        "    dz2 = a2 - y\n",
        "    dw2 = (1/m) * np.dot(a1.T, dz2)\n",
        "    db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "    dz1 = np.dot(dz2, w2.T) * (a1 > 0)  # ReLU derivative\n",
        "    dw1 = (1/m) * np.dot(X.T, dz1)\n",
        "    db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "    # Batch normalization gradients\n",
        "    dgamma2 = np.sum(dz2 * z2_hat, axis=0, keepdims=True)\n",
        "    dbeta2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "    dgamma1 = np.sum(dz1 * z1_hat, axis=0, keepdims=True)\n",
        "    dbeta1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "    return dw1, db1, dw2, db2, dgamma1, dbeta1, dgamma2, dbeta2\n",
        "\n",
        "# Step 9: Training loop\n",
        "learning_rate = 0.1\n",
        "num_epochs = 10000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        x_sample = X_train[i:i+1]\n",
        "        y_sample = y_train[i]\n",
        "\n",
        "        # Forward pass\n",
        "        a1, a2 = forward(x_sample)\n",
        "\n",
        "        # Compute and accumulate loss\n",
        "        loss = compute_loss(y_sample, a2)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation\n",
        "        dw1, db1, dw2, db2, dgamma1, dbeta1, dgamma2, dbeta2 = backward(x_sample, y_sample, a1, a2)\n",
        "\n",
        "        # Update weights, biases, and batch normalization parameters\n",
        "        w1 -= learning_rate * dw1\n",
        "        b1 -= learning_rate * db1\n",
        "        w2 -= learning_rate * dw2\n",
        "        b2 -= learning_rate * db2\n",
        "        gamma1 -= learning_rate * dgamma1\n",
        "        beta1 -= learning_rate * dbeta1\n",
        "        gamma2 -= learning_rate * dgamma2\n",
        "        beta2 -= learning_rate * dbeta2\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(X_train)}\")\n",
        "\n",
        "# Step 10: Evaluation\n",
        "_, y_pred_test = forward(X_test)  # Discard a1, keep only a2\n",
        "y_pred_test = (y_pred_test >= 0.5).astype(int)  # Convert to binary predictions\n",
        "accuracy = np.mean(y_pred_test == y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "K7I1kMF-gNn_",
        "outputId": "989be83d-5ff5-4730-8110-f9a86ea1ee93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-02e5f9480e9b>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgamma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgamma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Update weights, biases, and batch normalization parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-02e5f9480e9b>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(X, y, a1, a2)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Batch normalization gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdgamma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz2_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdgamma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz1_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'z2_hat' is not defined"
          ]
        }
      ]
    }
  ]
}